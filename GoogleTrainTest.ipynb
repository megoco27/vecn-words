{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train1 50212\n",
      "test1 48296\n",
      "trainsp0 48734\n",
      "train0 50257\n",
      "test0 50336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### Remove when our part works\n",
    "#data = pd.read_csv('../data/quora_duplicate_questions.tsv', sep='\\t')\n",
    "\n",
    "ItemsDF = pd.read_excel('Items.xlsx') #header=None, skiprows=1)\n",
    "leftItems  = ItemsDF[['VariableId','ItemId','Text','SourceId','PoolID']]\n",
    "rightItems = ItemsDF[['VariableId','ItemId','Text','SourceId','PoolID']]\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "train1 = 0\n",
    "test1 = 0\n",
    "trainsp0 = 0\n",
    "train0 = 0\n",
    "test0 = 0\n",
    "\n",
    "random.seed(1)\n",
    "for key, value in enumerate(leftItems.values):\n",
    "    for key2, value2 in enumerate(rightItems.values):\n",
    "        if value[1] < value2[1]:  #Only working with lower left triangle\n",
    "            if value[0] == value2[0]:  # Same variable (our 1s)  50,212 pairs\n",
    "                train.append([value[0]* value2[0],value[1],value2[1], value[2],value2[2],1])\n",
    "                train1 +=1\n",
    "            elif value[4] == value2[4]: #Same pool   #192,621 pairs\n",
    "                if random.randint(1,4) == 1:  #Downsampling test set 1s           \n",
    "                    test.append([value[0]* value2[0],value[1],value2[1], value[2],value2[2],1])\n",
    "                    test1+=1\n",
    "            elif value[3] == value2[3]: #Same paper 292,913 pairs\n",
    "                if random.randint(1,6) == 1:  #Downsampling same paper zeros class               \n",
    "                    train.append([value[0]* value2[0],value[1],value2[1], value[2],value2[2],0])\n",
    "                    trainsp0 +=1\n",
    "            else:  # Unrelated     158,552,457 pairs\n",
    "                if random.randint(1,3158) == 1:  #Picking zeros for train set             \n",
    "                    train.append([value[0]* value2[0],value[1],value2[1], value[2],value2[2],0])\n",
    "                    train0+=1\n",
    "                elif random.randint(1,3158) == 1:  #Picking zeros for testset        \n",
    "                    test.append([value[0]* value2[0],value[1],value2[1], value[2],value2[2],0])\n",
    "                    test0+=1\n",
    "\n",
    "print('train1',train1)\n",
    "print('test1',test1)\n",
    "print('trainsp0',trainsp0)\n",
    "print('train0',train0)\n",
    "print('test0',test0)\n",
    "\n",
    "\n",
    "                    \n",
    "train = pd.DataFrame(train)\n",
    "train = train.rename(columns = {0:'id', 1:'qid1',2:'quid2',3:'question1', 4:'question2',5:'is_duplicate'})\n",
    "\n",
    "test = pd.DataFrame(test)\n",
    "test = test.rename(columns = {0:'id', 1:'qid1',2:'quid2',3:'question1', 4:'question2',5:'is_duplicate'})\n",
    "\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train.to_csv('../data/train.csv', sep='\\t')\n",
    "test.to_csv('../data/test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17838"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new column in your existing dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the GoogleWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megoconnell/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an overlapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_words = [word for sentence in ItemsDF['Text'] for word in re.split('\\W', sentence)]\n",
    "unique = (set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = unique\n",
    "c = []\n",
    "for word in a:\n",
    "    if word in word_vectors.vocab:\n",
    "        c.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "for word in c:\n",
    "    vword = model[word]\n",
    "    vector = {word : vword }\n",
    "    dict1.update(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words1 = [word.lower() for sentence in ItemsDF['Text'] for word in re.split('\\W', sentence)]\n",
    "lower = (set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings1 = dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCosine1(W1,W2):\n",
    "    #resultww = []\n",
    "    result = 1 - spatial.distance.cosine(Embeddings1[W1], Embeddings1[W2])\n",
    "    #if W1.lower() in weightdict:\n",
    "        \n",
    "    result = round(result, 2)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSentenceSim1(S1,S2):\n",
    "    pairs = []\n",
    "    for index1,word1 in enumerate(S1):\n",
    "        for index2,word2 in enumerate(S2):\n",
    "            pairs.append([index1+1, word1,index2+1,word2,calculateCosine1(word1,word2)])\n",
    "    pairs = sorted(pairs, key=lambda x:x[4], reverse=True)\n",
    "    \n",
    "    # Now create the similarity vector\n",
    "    tempPairsVector = []\n",
    "    for index, row in enumerate(pairs):\n",
    "\n",
    "\n",
    "        foundOne=False\n",
    "        for tempRow in tempPairsVector:\n",
    "\n",
    "            if tempRow[0] == row[0] or tempRow[2] == row[2]:\n",
    "                foundOne=True\n",
    "        if foundOne==False:\n",
    "            tempPairsVector.append(row)\n",
    "\n",
    "#Now create the vector of the up to 20 highest cosines\n",
    "    sentenceSimVector = [] #[None] * 20\n",
    "    for index,row in enumerate(tempPairsVector):\n",
    "        sentenceSimVector.append(row[4])\n",
    "    \n",
    "#Pad the similarity vector with nulls    \n",
    "    length = 20 - len(sentenceSimVector)\n",
    "    for x in range(0, length):\n",
    "        sentenceSimVector.append(None)            \n",
    "    return(sentenceSimVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dictionary from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>quid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2558869596</td>\n",
       "      <td>28852</td>\n",
       "      <td>67266</td>\n",
       "      <td>I believe that it is my personal duty as CAPS'...</td>\n",
       "      <td>To accomplish their work, employees are willin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   qid1  quid2  \\\n",
       "0  2558869596  28852  67266   \n",
       "\n",
       "                                           question1  \\\n",
       "0  I believe that it is my personal duty as CAPS'...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  To accomplish their work, employees are willin...             0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintest = train.head(20)\n",
    "traintest.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "new = []\n",
    "for index, row in train.iterrows():\n",
    "        itemslist1 = [row['question1']]\n",
    "        itemslist2 = [row['question2']]\n",
    "        #print(itemslist1)\n",
    "        #print(itemslist2)\n",
    "        words1 = ([word for sentence in itemslist1 for word in tokenizer.tokenize(sentence)])\n",
    "        words2 = ([word for sentence in itemslist2 for word in tokenizer.tokenize(sentence)])\n",
    "        #print(words1,words2)\n",
    "        filtered_sentence1 = [w for w in words1 if not w in stop_words]\n",
    "        filtered_sentence2 = [w for w in words2 if not w in stop_words]\n",
    "        #print(filtered_sentence1,filtered_sentence2)\n",
    "        for word in filtered_sentence1:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence1.remove(word)\n",
    "        for word in filtered_sentence1:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence1.remove(word)\n",
    "        for word in filtered_sentence2:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence2.remove(word)\n",
    "        for word in filtered_sentence2:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence2.remove(word)     \n",
    "        new.append([row['id'],row['qid1'], row['quid2'],row['is_duplicate'],calculateSentenceSim1(filtered_sentence1,filtered_sentence2)])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "newt = []\n",
    "for index, row in test.iterrows():\n",
    "        itemslist1 = [row['question1']]\n",
    "        itemslist2 = [row['question2']]\n",
    "        #print(itemslist1)\n",
    "        #print(itemslist2)\n",
    "        words1 = ([word for sentence in itemslist1 for word in tokenizer.tokenize(sentence)])\n",
    "        words2 = ([word for sentence in itemslist2 for word in tokenizer.tokenize(sentence)])\n",
    "        #print(words1,words2)\n",
    "        filtered_sentence1 = [w for w in words1 if not w in stop_words]\n",
    "        filtered_sentence2 = [w for w in words2 if not w in stop_words]\n",
    "        #print(filtered_sentence1,filtered_sentence2)\n",
    "        for word in filtered_sentence1:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence1.remove(word)\n",
    "        for word in filtered_sentence1:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence1.remove(word)\n",
    "        for word in filtered_sentence2:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence2.remove(word)\n",
    "        for word in filtered_sentence2:\n",
    "            if word not in Embeddings1.keys():\n",
    "                filtered_sentence2.remove(word)     \n",
    "        newt.append([row['id'],row['qid1'], row['quid2'],row['is_duplicate'],calculateSentenceSim1(filtered_sentence1,filtered_sentence2)])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50212 trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2558869596,\n",
       " 28852,\n",
       " 67266,\n",
       " 0,\n",
       " 1.0,\n",
       " 0.38,\n",
       " 0.34,\n",
       " 0.28,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.2,\n",
       " 0.14,\n",
       " 0.14,\n",
       " 0.13,\n",
       " 0.12,\n",
       " 0.07,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = []\n",
    "list1.append([])\n",
    "list1 = []\n",
    "list1.append([])\n",
    "z = 0\n",
    "for x in new:\n",
    "    list2 = []\n",
    "    for y in x[4]:\n",
    "        \n",
    "        #print(y)\n",
    "        #print(x[0],x[1],x[2], x[3],y)\n",
    "        list2.append(y)\n",
    "        #for m in list2:\n",
    "            #list3.append(m)\n",
    "            #list3[z].extend(m)\n",
    "    list1[z].append(x[0])\n",
    "    list1[z].append(x[1])\n",
    "    list1[z].append(x[2])\n",
    "    list1[z].append(x[3])\n",
    "    list1[z].extend(list2)\n",
    "    list1[z]\n",
    "    z += 1\n",
    "    list1.append([])\n",
    "list1[0]\n",
    "#list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6649157254,\n",
       " 60393,\n",
       " 60836,\n",
       " 0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.68,\n",
       " 0.18,\n",
       " 0.16,\n",
       " 0.14,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listT1 = []\n",
    "listT1.append([])\n",
    "listT1 = []\n",
    "listT1.append([])\n",
    "z = 0\n",
    "for x in newt:\n",
    "    list2 = []\n",
    "    for y in x[4]:\n",
    "        \n",
    "        #print(y)\n",
    "        #print(x[0],x[1],x[2], x[3],y)\n",
    "        list2.append(y)\n",
    "        #for m in list2:\n",
    "            #list3.append(m)\n",
    "            #list3[z].extend(m)\n",
    "    listT1[z].append(x[0])\n",
    "    listT1[z].append(x[1])\n",
    "    listT1[z].append(x[2])\n",
    "    listT1[z].append(x[3])\n",
    "    listT1[z].extend(list2)\n",
    "    listT1[z]\n",
    "    z += 1\n",
    "    listT1.append([])\n",
    "listT1[0]\n",
    "#list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableDF2 = pd.DataFrame(list1)\n",
    "tableDF2T = pd.DataFrame(listT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tableDF2 = tableDF2.rename(columns = {0:'id', 1:'quid1',2:'quid2',3:'Target'})\n",
    "tableDF2T = tableDF2T.rename(columns = {0:'id', 1:'quid1',2:'quid2',3:'Target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleNormal = tableDF2.iloc[:,0:24]\n",
    "GoogleNormalT = tableDF2T.iloc[:,0:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleNormal.to_csv('GooglePoolTrain.csv')\n",
    "GoogleNormalT.to_csv('GooglePoolTest.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
